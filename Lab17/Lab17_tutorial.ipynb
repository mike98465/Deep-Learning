{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input Size\n",
    "IMG_WIDTH = 84\n",
    "IMG_HEIGHT = 84\n",
    "NUM_STACK = 4\n",
    "# For Epsilon-greedy\n",
    "MIN_EXPLORING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name, num_action, discount_factor=0.99):\n",
    "        self.exploring_rate = 0.1\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "        self.model = self.build_model(name)\n",
    "\n",
    "    def build_model(self, name):\n",
    "        # input: state\n",
    "        # output: each action's Q-value \n",
    "        screen_stack = tf.keras.Input(shape=[IMG_WIDTH, IMG_HEIGHT, NUM_STACK], dtype=tf.float32)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(filters=32, kernel_size=8, strides=4)(screen_stack)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=4, strides=2)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(units=512)(x)\n",
    "        x = tf.keras.layers.ReLU()(x)\n",
    "        Q = tf.keras.layers.Dense(self.num_action)(x)\n",
    "\n",
    "        model = tf.keras.Model(name=name, inputs=screen_stack, outputs=Q)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def loss(self, state, action, reward, tar_Q, ternimal):\n",
    "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "        output = self.model(state)\n",
    "        index = tf.stack([tf.range(tf.shape(action)[0]), action], axis=1)\n",
    "        # Q(s,a,theta) for selected a, shape (batch_size, 1)\n",
    "        Q = tf.gather_nd(output, index)\n",
    "        \n",
    "        # set tar_Q as 0 if reaching terminal state\n",
    "        tar_Q *= ~np.array(terminal)\n",
    "\n",
    "        # loss = E[r+max(Q(s',a',theta'))-Q(s,a,theta)]\n",
    "        loss = tf.reduce_mean(tf.square(reward + self.discount_factor * tar_Q - Q))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def max_Q(self, state):\n",
    "        # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "        output = self.model(state)\n",
    "\n",
    "        # max(Q(s',a',theta')), shape (batch_size, 1)\n",
    "        return tf.reduce_max(output, axis=1)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(self.num_action)  # Select a random action\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis = 0)\n",
    "            # Q(s,a,theta) for all a, shape (batch_size, num_action)\n",
    "            output = self.model(state)\n",
    "\n",
    "            # select action with highest action-value\n",
    "            action = tf.argmax(output, axis=1)[0]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self, episode):\n",
    "        self.exploring_rate = max(MIN_EXPLORING_RATE, min(0.5, 0.99**((episode) / 30)))\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init agent\n",
    "num_action = len(env.getActionSet())\n",
    "\n",
    "# agent for frequently updating\n",
    "online_agent = Agent('online', num_action)\n",
    "\n",
    "# agent for slow updating\n",
    "target_agent = Agent('target', num_action)\n",
    "# synchronize target model's weight with online model's weight\n",
    "target_agent.model.set_weights(online_agent.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "average_loss = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "@tf.function\n",
    "def train_step(state, action, reward, next_state, ternimal):\n",
    "    # Delayed Target Network\n",
    "    tar_Q = target_agent.max_Q(next_state)\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = online_agent.loss(state, action, reward, tar_Q, ternimal)\n",
    "    gradients = tape.gradient(loss, online_agent.model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, online_agent.model.trainable_variables))\n",
    "    \n",
    "    average_loss.update_state(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.experiences = []\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def add(self, experience):\n",
    "        if len(self.experiences) >= self.buffer_size:\n",
    "            self.experiences.pop(0)\n",
    "        self.experiences.append(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        \"\"\"\n",
    "        sample experience from buffer\n",
    "        \"\"\"\n",
    "        if size > len(self.experiences):\n",
    "            experiences_idx = np.random.choice(len(self.experiences), size=size)\n",
    "        else:\n",
    "            experiences_idx = np.random.choice(len(self.experiences), size=size, replace=False)\n",
    "\n",
    "        # from all sampled experiences, extract a tuple of (s,a,r,s')\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_prime = []\n",
    "        terminal = []\n",
    "        for i in range(size):\n",
    "            states.append(self.experiences[experiences_idx[i]][0])\n",
    "            actions.append(self.experiences[experiences_idx[i]][1])\n",
    "            rewards.append(self.experiences[experiences_idx[i]][2])\n",
    "            states_prime.append(self.experiences[experiences_idx[i]][3])\n",
    "            terminal.append(self.experiences[experiences_idx[i]][4])\n",
    "\n",
    "        return states, actions, rewards, states_prime, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init buffer\n",
    "buffer = Replay_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.transform\n",
    "\n",
    "def preprocess_screen(screen):\n",
    "    screen = skimage.transform.resize(screen, [IMG_WIDTH, IMG_HEIGHT, 1])\n",
    "    return screen\n",
    "\n",
    "def frames_to_state(input_frames):\n",
    "    if(len(input_frames) == 1):\n",
    "        state = np.concatenate(input_frames*4, axis=-1)\n",
    "    elif(len(input_frames) == 2):\n",
    "        state = np.concatenate(input_frames[0:1]*2 + input_frames[1:]*2, axis=-1)\n",
    "    elif(len(input_frames) == 3):\n",
    "        state = np.concatenate(input_frames + input_frames[2:], axis=-1)\n",
    "    else:\n",
    "        state = np.concatenate(input_frames[-4:], axis=-1)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "update_every_iteration = 1000\n",
    "print_every_episode = 500\n",
    "save_video_every_episode = 5000\n",
    "NUM_EPISODE = 20000\n",
    "NUM_EXPLORE = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "iter_num = 0\n",
    "for episode in range(0, NUM_EPISODE + 1):\n",
    "    \n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "    \n",
    "    # record frame\n",
    "    if episode % save_video_every_episode == 0:\n",
    "        frames = [env.getScreenRGB()]\n",
    "    \n",
    "    # input frame\n",
    "    input_frames = [preprocess_screen(env.getScreenGrayscale())]\n",
    "    \n",
    "    # for every 500 episodes, shutdown exploration to see the performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        online_agent.shutdown_explore()\n",
    "    \n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0\n",
    "    \n",
    "    t = 0\n",
    "    while not env.game_over():\n",
    "        \n",
    "        state = frames_to_state(input_frames)\n",
    "        \n",
    "        # feed current state and select an action\n",
    "        action = online_agent.select_action(state)\n",
    "        \n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "        \n",
    "        # record frame\n",
    "        if episode % save_video_every_episode == 0:\n",
    "            frames.append(env.getScreenRGB())\n",
    "        \n",
    "        # record input frame\n",
    "        input_frames.append(preprocess_screen(env.getScreenGrayscale()))\n",
    "        \n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "        \n",
    "        # observe the result\n",
    "        state_prime = frames_to_state(input_frames)  # get next state\n",
    "        \n",
    "        # append experience for this episode\n",
    "        if episode % print_every_episode != 0:\n",
    "            buffer.add((state, action, reward, state_prime, env.game_over()))\n",
    "        \n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "        t += 1\n",
    "        \n",
    "        # update agent\n",
    "        if episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
    "            iter_num += 1\n",
    "            train_states, train_actions, train_rewards, train_states_prime, terminal = buffer.sample(BATCH_SIZE)\n",
    "            train_states = np.asarray(train_states).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "            train_states_prime = np.asarray(train_states_prime).reshape(-1, IMG_WIDTH, IMG_HEIGHT, NUM_STACK)\n",
    "            \n",
    "            # convert Python object to Tensor to prevent graph re-tracing\n",
    "            train_states = tf.convert_to_tensor(train_states, tf.float32)\n",
    "            train_actions = tf.convert_to_tensor(train_actions, tf.int32)\n",
    "            train_rewards = tf.convert_to_tensor(train_rewards, tf.float32)\n",
    "            train_states_prime = tf.convert_to_tensor(train_states_prime, tf.float32)\n",
    "            terminal = tf.convert_to_tensor(terminal, tf.bool)\n",
    "            \n",
    "            train_step(train_states, train_actions, train_rewards, train_states_prime, terminal)\n",
    "\n",
    "        # synchronize target model's weight with online model's weight every 1000 iterations\n",
    "        if iter_num % update_every_iteration == 0 and episode > NUM_EXPLORE and episode % print_every_episode != 0:\n",
    "            target_agent.model.set_weights(online_agent.model.get_weights())\n",
    "\n",
    "    # update exploring rate\n",
    "    online_agent.update_parameters(episode)\n",
    "    target_agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0 and episode > NUM_EXPLORE:\n",
    "        print(\n",
    "            \"[{}] time live:{}, cumulated reward: {}, exploring rate: {}, average loss: {}\".\n",
    "            format(episode, t, cum_reward, online_agent.exploring_rate, average_loss.result()))\n",
    "        average_loss.reset_states()\n",
    "\n",
    "    if episode % save_video_every_episode == 0:  # for every 500 episode, record an animation\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        clip.write_videofile(\"movie_f/DQN_demo-{}.webm\".format(episode), fps=60)\n",
    "#         display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "clip = VideoFileClip(\"movie_f/DQN_demo-20000.webm\")\n",
    "display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
