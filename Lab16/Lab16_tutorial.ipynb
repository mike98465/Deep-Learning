{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# four actions in the game\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the top left or the bottom right corner.\n",
    "    For example, a 4x4 grid looks as follows:\n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "    x is your position and T are the two terminal states.\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "    Actions going off the edge leave you in your current state.\n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self, shape=[4, 4]):\n",
    "        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
    "            raise ValueError('shape argument must be a list/tuple of length 2')\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "        nS = np.prod(shape)\n",
    "        nA = 4\n",
    "\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "\n",
    "        P = {}\n",
    "        grid = np.arange(nS).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "\n",
    "            P[s] = {a: [] for a in range(nA)}\n",
    "\n",
    "            is_done = lambda s: s == 0 or s == (nS - 1)\n",
    "            reward = 0.0 if is_done(s) else -1.0\n",
    "\n",
    "            # We're stuck in a terminal state\n",
    "            if is_done(s):\n",
    "                P[s][UP] = [(1.0, s, reward, True)]\n",
    "                P[s][RIGHT] = [(1.0, s, reward, True)]\n",
    "                P[s][DOWN] = [(1.0, s, reward, True)]\n",
    "                P[s][LEFT] = [(1.0, s, reward, True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                ns_up = s if y == 0 else s - MAX_X\n",
    "                ns_right = s if x == (MAX_X - 1) else s + 1\n",
    "                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
    "                ns_left = s if x == 0 else s - 1\n",
    "                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
    "                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
    "                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
    "                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(nS) / nS\n",
    "\n",
    "        # We expose the model of the environment for educational purposes\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.P = P\n",
    "\n",
    "        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        grid = np.arange(self.nS).reshape(self.shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "        while not it.finished:\n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            elif s == 0 or s == self.nS - 1:\n",
    "                output = \" T \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if x == 0:\n",
    "                output = output.lstrip()\n",
    "            if x == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "\n",
    "            outfile.write(output)\n",
    "\n",
    "            if x == self.shape[1] - 1:\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "            it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Given an state, calculate the new value function V(s) based on the value iteration algorithm\n",
    "        Args:\n",
    "            state: represents each state in the Gridworld, an integer\n",
    "            V: the current value function of the states(V(s)), the lengh is env.nS\n",
    "\n",
    "        Returns:\n",
    "            a new V(s)\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function\n",
    "            V[s] = best_action_value\n",
    "            # Check if we can stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    return policy, V\n",
    "\n",
    "\n",
    "policy, v = value_iteration(env)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from gym.envs.toy_text import discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "            \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the current policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, v = policy_improvement(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n",
    "game = FlappyBird()\n",
    "env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n",
    "env.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a dictionary whose key is action description and value is action index\n",
    "print(game.actions)\n",
    "# return a list of action index (include None)\n",
    "print(env.getActionSet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary describe state\n",
    "'''\n",
    "    player y position.\n",
    "    players velocity.\n",
    "    next pipe distance to player\n",
    "    next pipe top y position\n",
    "    next pipe bottom y position\n",
    "    next next pipe distance to player\n",
    "    next next pipe top y position\n",
    "    next next pipe bottom y position\n",
    "'''\n",
    "game.getGameState()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "MIN_EXPLORING_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.5\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 bucket_range_per_feature,\n",
    "                 num_action,\n",
    "                 t=0,\n",
    "                 discount_factor=0.99):\n",
    "        self.update_parameters(t)  # init explore rate and learning rate\n",
    "        self.q_table = defaultdict(lambda: np.zeros(num_action))\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_action = num_action\n",
    "\n",
    "        # how to discretize each feature in a state\n",
    "        # the higher each value, less time to train but with worser performance\n",
    "        # e.g. if range = 2, feature with value 1 is equal to feature with value 0 bacause int(1/2) = int(0/2)\n",
    "        self.bucket_range_per_feature = bucket_range_per_feature\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        state_idx = self.get_state_idx(state)\n",
    "        if np.random.rand() < self.exploring_rate:\n",
    "            action = np.random.choice(num_action)  # Select a random action\n",
    "        else:\n",
    "            action = np.argmax(\n",
    "                self.q_table[state_idx])  # Select the action with the highest q\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, state, action, reward, state_prime):\n",
    "        state_idx = self.get_state_idx(state)\n",
    "        state_prime_idx = self.get_state_idx(state_prime)\n",
    "        # Update Q_value using Q-learning update rule\n",
    "        best_q = np.max(self.q_table[state_prime_idx])\n",
    "        self.q_table[state_idx][action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * best_q - self.q_table[state_idx][action])\n",
    "\n",
    "    def get_state_idx(self, state):\n",
    "        # instead of using absolute position of pipe, use relative position\n",
    "        state = copy.deepcopy(state)\n",
    "        state['next_next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_next_pipe_top_y'] -= state['player_y']\n",
    "        state['next_pipe_bottom_y'] -= state['player_y']\n",
    "        state['next_pipe_top_y'] -= state['player_y']\n",
    "\n",
    "        # sort to make list converted from dict ordered in alphabet order\n",
    "        state_key = [k for k, v in sorted(state.items())]\n",
    "\n",
    "        # do bucketing to decrease state space to speed up training\n",
    "        state_idx = []\n",
    "        for key in state_key:\n",
    "            state_idx.append(\n",
    "                int(state[key] / self.bucket_range_per_feature[key]))\n",
    "        return tuple(state_idx)\n",
    "\n",
    "    def update_parameters(self, episode):\n",
    "        self.exploring_rate = max(MIN_EXPLORING_RATE,\n",
    "                                  min(0.5, 0.99**((episode) / 30)))\n",
    "        self.learning_rate = max(MIN_LEARNING_RATE, min(0.5, 0.99\n",
    "                                                        ** ((episode) / 30)))\n",
    "\n",
    "    def shutdown_explore(self):\n",
    "        # make action selection greedy\n",
    "        self.exploring_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_action = len(env.getActionSet())\n",
    "bucket_range_per_feature = {\n",
    "  'next_next_pipe_bottom_y': 40,\n",
    "  'next_next_pipe_dist_to_player': 512,\n",
    "  'next_next_pipe_top_y': 40,\n",
    "  'next_pipe_bottom_y': 20,\n",
    "  'next_pipe_dist_to_player': 20,\n",
    "  'next_pipe_top_y': 20,\n",
    "  'player_vel': 4,\n",
    "  'player_y': 16\n",
    "}\n",
    "# init agent\n",
    "agent = Agent(bucket_range_per_feature, num_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "\n",
    "\n",
    "def make_anim(images, fps=60, true_image=False):\n",
    "    duration = len(images) / fps\n",
    "\n",
    "    def make_frame(t):\n",
    "        try:\n",
    "            x = images[int(len(images) / duration * t)]\n",
    "        except:\n",
    "            x = images[-1]\n",
    "\n",
    "        if true_image:\n",
    "            return x.astype(np.uint8)\n",
    "        else:\n",
    "            return ((x + 1) / 2 * 255).astype(np.uint8)\n",
    "\n",
    "    clip = mpy.VideoClip(make_frame, duration=duration)\n",
    "    clip.fps = fps\n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "reward_per_epoch = []\n",
    "lifetime_per_epoch = []\n",
    "exploring_rates = []\n",
    "learning_rates = []\n",
    "print_every_episode = 500\n",
    "show_gif_every_episode = 5000\n",
    "NUM_EPISODE = 40000\n",
    "for episode in range(0, NUM_EPISODE):\n",
    "\n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "\n",
    "    # record frame\n",
    "    frames = [env.getScreenRGB()]\n",
    "\n",
    "    # for every 500 episodes, shutdown exploration to see performance of greedy action\n",
    "    if episode % print_every_episode == 0:\n",
    "        agent.shutdown_explore()\n",
    "\n",
    "    # the initial state\n",
    "    state = game.getGameState()\n",
    "    # cumulate reward for this episode\n",
    "    cum_reward = 0  \n",
    "    t = 0\n",
    "\n",
    "    while not env.game_over():\n",
    "\n",
    "        # select an action\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # execute the action and get reward\n",
    "        # reward = +1 when pass a pipe, -5 when die\n",
    "        reward = env.act(env.getActionSet()[action])  \n",
    "\n",
    "        frames.append(env.getScreenRGB())\n",
    "\n",
    "        # cumulate reward\n",
    "        cum_reward += reward\n",
    "\n",
    "        # observe the result\n",
    "        state_prime = game.getGameState()  # get next state\n",
    "\n",
    "        # update agent\n",
    "        agent.update_policy(state, action, reward, state_prime)\n",
    "\n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "        t += 1\n",
    "\n",
    "    # update exploring_rate and learning_rate\n",
    "    agent.update_parameters(episode)\n",
    "\n",
    "    if episode % print_every_episode == 0:\n",
    "        print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "        print(\"cumulated reward: %f\" % cum_reward)\n",
    "        print(\"exploring rate %f\" % agent.exploring_rate)\n",
    "        print(\"learning rate %f\" % agent.learning_rate)\n",
    "        reward_per_epoch.append(cum_reward)\n",
    "        exploring_rates.append(agent.exploring_rate)\n",
    "        learning_rates.append(agent.learning_rate)\n",
    "        lifetime_per_epoch.append(t)\n",
    "\n",
    "    # for every 5000 episode, record an animation\n",
    "    if episode % show_gif_every_episode == 0:\n",
    "        print(\"len frames:\", len(frames))\n",
    "        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "        display(clip.ipython_display(fps=60, autoplay=1, loop=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo():\n",
    "    # Reset the environment\n",
    "    env.reset_game()\n",
    "\n",
    "    # record frame\n",
    "    frames = [env.getScreenRGB()]\n",
    "\n",
    "    # shutdown exploration to see performance of greedy action\n",
    "    agent.shutdown_explore()\n",
    "\n",
    "    # the initial state\n",
    "    state = game.getGameState()\n",
    "\n",
    "    while not env.game_over():\n",
    "        # select an action\n",
    "        action = agent.select_action(state)\n",
    "\n",
    "        # execute the action and get reward\n",
    "        reward = env.act(env.getActionSet()[action])\n",
    "\n",
    "        frames.append(env.getScreenRGB())\n",
    "\n",
    "        # observe the result\n",
    "        state_prime = game.getGameState()  # get next state\n",
    "\n",
    "        # Setting up for the next iteration\n",
    "        state = state_prime\n",
    "\n",
    "    clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n",
    "    display(clip.ipython_display(fps=60, autoplay=1, loop=1))\n",
    "\n",
    "\n",
    "demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot life time against training episodes\n",
    "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
    "plt.plot(range(len(lifetime_per_epoch)), lifetime_per_epoch)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward against training episodes\n",
    "fig, ax1 = plt.subplots(figsize=(20, 5))\n",
    "plt.plot(range(len(reward_per_epoch)), reward_per_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nthu-datalab.github.io/ml/labs/16_Q-Learning/16_Q-Learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
